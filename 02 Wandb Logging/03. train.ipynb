{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31cf0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from data import DataModule\n",
    "from model import ColaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef5d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplesVisualisationLogger(pl.Callback):\n",
    "    def __init__(self, datamodule):\n",
    "        super().__init__()\n",
    "\n",
    "        self.datamodule = datamodule\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        val_batch = next(iter(self.datamodule.val_dataloader()))\n",
    "        sentences = val_batch[\"sentence\"]\n",
    "\n",
    "        outputs = pl_module(val_batch[\"input_ids\"], val_batch[\"attention_mask\"])\n",
    "        preds = torch.argmax(outputs.logits, 1)\n",
    "        labels = val_batch[\"label\"]\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\"Sentence\": sentences, \"Label\": labels.numpy(), \"Predicted\": preds.numpy()}\n",
    "        )\n",
    "\n",
    "        wrong_df = df[df[\"Label\"] != df[\"Predicted\"]]\n",
    "        trainer.logger.experiment.log(\n",
    "            {\n",
    "                \"examples\": wandb.Table(dataframe=wrong_df, allow_mixed_types=True),\n",
    "                \"global_step\": trainer.global_step,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca46127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cola_data = DataModule()\n",
    "    cola_model = ColaModel()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"./models\",\n",
    "        filename=\"best-checkpoint.ckpt\",\n",
    "        monitor=\"valid/loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"valid/loss\", patience=3, verbose=True, mode=\"min\"\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"MLOPS AWS\", entity=\"techwithshadab\")\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=1,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[checkpoint_callback, SamplesVisualisationLogger(cola_data), early_stopping_callback],\n",
    "        log_every_n_steps=10,\n",
    "        deterministic=True,\n",
    "        # limit_train_batches=0.25,\n",
    "        # limit_val_batches=0.25\n",
    "    )\n",
    "    trainer.fit(cola_model, cola_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c9fd16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Reusing dataset glue (C:\\Users\\bhatt\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 9/9 [00:03<00:00,  2.50ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.88ba/s]\n",
      "\n",
      "  | Name                   | Type                          | Params\n",
      "-------------------------------------------------------------------------\n",
      "0 | bert                   | BertForSequenceClassification | 4.4 M \n",
      "1 | train_accuracy_metric  | Accuracy                      | 0     \n",
      "2 | val_accuracy_metric    | Accuracy                      | 0     \n",
      "3 | f1_metric              | F1                            | 0     \n",
      "4 | precision_macro_metric | Precision                     | 0     \n",
      "5 | recall_macro_metric    | Recall                        | 0     \n",
      "6 | precision_micro_metric | Precision                     | 0     \n",
      "7 | recall_micro_metric    | Recall                        | 0     \n",
      "-------------------------------------------------------------------------\n",
      "4.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.4 M     Total params\n",
      "17.545    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 151/151 [04:37<00:00,  1.84s/it, loss=0.62, v_num=0ovl, valid/loss_epoch=0.618, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.523, train/acc_step=0.795, train/loss_epoch=0.612, train/acc_epoch=0.704, valid/loss_step=0.488]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c956ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "253ffdc4e2d9fa0324d7484d8cb29c79e14c32a12603a00e0217607efe1b20d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
