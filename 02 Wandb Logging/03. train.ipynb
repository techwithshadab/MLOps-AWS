{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31cf0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from data import DataModule\n",
    "from model import ColaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef5d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplesVisualisationLogger(pl.Callback):\n",
    "    def __init__(self, datamodule):\n",
    "        super().__init__()\n",
    "\n",
    "        self.datamodule = datamodule\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        val_batch = next(iter(self.datamodule.val_dataloader()))\n",
    "        sentences = val_batch[\"sentence\"]\n",
    "\n",
    "        outputs = pl_module(val_batch[\"input_ids\"], val_batch[\"attention_mask\"])\n",
    "        preds = torch.argmax(outputs.logits, 1)\n",
    "        labels = val_batch[\"label\"]\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\"Sentence\": sentences, \"Label\": labels.numpy(), \"Predicted\": preds.numpy()}\n",
    "        )\n",
    "\n",
    "        wrong_df = df[df[\"Label\"] != df[\"Predicted\"]]\n",
    "        trainer.logger.experiment.log(\n",
    "            {\n",
    "                \"examples\": wandb.Table(dataframe=wrong_df, allow_mixed_types=True),\n",
    "                \"global_step\": trainer.global_step,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca46127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cola_data = DataModule()\n",
    "    cola_model = ColaModel()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"./models\",\n",
    "        filename=\"best-checkpoint.ckpt\",\n",
    "        monitor=\"valid/loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"valid/loss\", patience=3, verbose=True, mode=\"min\"\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"MLOPS-AWS\", entity=\"madihaij\")\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=1,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[checkpoint_callback, SamplesVisualisationLogger(cola_data), early_stopping_callback],\n",
    "        log_every_n_steps=10,\n",
    "        deterministic=True,\n",
    "        # limit_train_batches=0.25,\n",
    "        # limit_val_batches=0.25\n",
    "    )\n",
    "    trainer.fit(cola_model, cola_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9fd16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Reusing dataset glue (/Users/madihaijaz/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.70ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 37.07ba/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmadihaij\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1260a0b727455e812e602f66422da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.03348924318949382, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/madihaijaz/MLOps-AWS-1/02 Wandb Logging/wandb/run-20220930_122507-34sgj92s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/madihaij/MLOPS-AWS/runs/34sgj92s\" target=\"_blank\">vivid-cherry-1</a></strong> to <a href=\"https://wandb.ai/madihaij/MLOPS-AWS\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                   | Type                          | Params\n",
      "-------------------------------------------------------------------------\n",
      "0 | bert                   | BertForSequenceClassification | 4.4 M \n",
      "1 | train_accuracy_metric  | Accuracy                      | 0     \n",
      "2 | val_accuracy_metric    | Accuracy                      | 0     \n",
      "3 | f1_metric              | F1                            | 0     \n",
      "4 | precision_macro_metric | Precision                     | 0     \n",
      "5 | recall_macro_metric    | Recall                        | 0     \n",
      "6 | precision_micro_metric | Precision                     | 0     \n",
      "7 | recall_micro_metric    | Recall                        | 0     \n",
      "-------------------------------------------------------------------------\n",
      "4.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.4 M     Total params\n",
      "17.545    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madihaijaz/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/151 [00:00<?, ?it/s]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madihaijaz/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 151/151 [01:42<00:00,  1.47it/s, loss=0.601, v_num=j92s, valid/loss_epoch=0.617, valid/acc=0.691, valid/precision_macro=0.346, valid/recall_macro=0.500, valid/precision_micro=0.691, valid/recall_micro=0.691, valid/f1=0.691, train/loss_step=0.528, train/acc_step=0.795, train/loss_epoch=0.615, train/acc_epoch=0.700, valid/loss_step=0.480]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6f6267de85928af9250923ba3e7b5ae95a40ac2651367fe538311ce224174eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
