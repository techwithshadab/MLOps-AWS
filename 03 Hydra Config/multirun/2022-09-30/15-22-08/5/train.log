[2022-09-30 15:33:48,770][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
processing:
  batch_size: 128
  max_length: 128
training:
  max_epochs: 2
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25
wandb:
  project: MLOPS AWS
  entity: techwithshadab

[2022-09-30 15:33:48,772][__main__][INFO] - Using the model: google/bert_uncased_L-2_H-128_A-2
[2022-09-30 15:33:48,773][__main__][INFO] - Using the tokenizer: google/bert_uncased_L-2_H-128_A-2
[2022-09-30 15:33:59,484][datasets.builder][WARNING] - Reusing dataset glue (C:\Users\bhatt\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
