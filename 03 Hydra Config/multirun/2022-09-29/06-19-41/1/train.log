[2022-09-29 06:20:25,609][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
processing:
  batch_size: 64
  max_length: 128
training:
  max_epochs: 2
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25

[2022-09-29 06:20:25,610][__main__][INFO] - Using the model: google/bert_uncased_L-2_H-128_A-2
[2022-09-29 06:20:25,610][__main__][INFO] - Using the tokenizer: google/bert_uncased_L-2_H-128_A-2
[2022-09-29 06:20:34,541][datasets.builder][WARNING] - Reusing dataset glue (C:\Users\bhatt\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
